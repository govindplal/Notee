{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T19:06:50.874478Z","iopub.execute_input":"2025-11-30T19:06:50.874715Z","iopub.status.idle":"2025-11-30T19:06:52.696643Z","shell.execute_reply.started":"2025-11-30T19:06:50.874689Z","shell.execute_reply":"2025-11-30T19:06:52.695783Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/agents-intensive-capstone-project/Hackathon dataset.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# **Notee Insight Agent**\n**A Cognitive AI system for Knowledge Graph Reasoning & Insight Generator**\\\n\nThis notebook implements a *second brain* agent that:\n1. Generates a synthetic dataset of personal notes and interactions\n2. Uses Gemini embeddings to embed notes ans build a semantic memory space\n3. Builds a personal knowledge graph over notes + tags + time\n4. Runs an insight engine that detects:\n       - Novel connections between distant notes\n       - Revived old notes\n       - Topic drift over time\n5. Uses Gemini as:\n       - A reflection agent that explains the insights and suggests next steps.\n       - A RAG Q&A Layer over notes","metadata":{}},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab kfp 2>/dev/null\n!pip install -q faker umap-learn networkx\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T19:10:15.419112Z","iopub.execute_input":"2025-11-30T19:10:15.419427Z","iopub.status.idle":"2025-11-30T19:10:24.646032Z","shell.execute_reply.started":"2025-11-30T19:10:15.419396Z","shell.execute_reply":"2025-11-30T19:10:24.645094Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import random\nimport uuid\nimport math\nfrom datetime import datetime, timedelta\nfrom collections import Counter\n\nfrom faker import Faker\nfrom dateutil.relativedelta import relativedelta\n\nimport networkx as nx\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nimport umap\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nimport json\n\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nFaker.seed(RANDOM_SEED)\nfake = Faker()\n\nplt.rcParams[\"figure.figsize\"] = (10, 7)\n\nprint(\"Imports ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T19:12:54.649595Z","iopub.execute_input":"2025-11-30T19:12:54.649914Z","iopub.status.idle":"2025-11-30T19:12:54.658154Z","shell.execute_reply.started":"2025-11-30T19:12:54.649875Z","shell.execute_reply":"2025-11-30T19:12:54.657262Z"}},"outputs":[{"name":"stdout","text":"Imports ready\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"##Gemini configuration\nfrom google import genai\nfrom google.genai import types\n\n##Retry\nfrom google.api_core import retry\n\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\ngenai.models.Models.generate_content = retry.Retry(\n    predicate=is_retriable)(genai.models.Models.generate_content)\n\n\n\nfrom kaggle_secrets import UserSecretsClient\n\nGEMINI_API_KEY = UserSecretsClient().get_secret(\"GEMINI_API_KEY\")\n\nclient = genai.Client(api_key=GEMINI_API_KEY)\n\nEMBED_MODEL = \"models/text-embedding-004\"\n\nGEN_MODEL = \"gemini-2.0-flash\"\n\nprint(\"Gemini configured\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T19:18:12.269764Z","iopub.execute_input":"2025-11-30T19:18:12.270113Z","iopub.status.idle":"2025-11-30T19:18:12.471353Z","shell.execute_reply.started":"2025-11-30T19:18:12.270090Z","shell.execute_reply":"2025-11-30T19:18:12.470541Z"}},"outputs":[{"name":"stdout","text":"Gemini configured\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"**Generate Synthetic Notes & Interactions**\n \nWe create:\n \n- `notes_df` – personal notes with:\n  - title, content, tags, topic, context, timestamp\n- `interactions_df` – events like opening, editing, and revisiting notes","metadata":{}},{"cell_type":"code","source":"# ==== Synthetic Data Generation ====\n \nN_NOTES = 400\nMONTHS_SPAN = 9\n \nSTART_DATE = datetime.utcnow() - relativedelta(months=MONTHS_SPAN)\n \nTAG_POOL = [\n    \"ai\", \"product\", \"research\", \"design\", \"meeting\", \"personal\", \"health\",\n    \"finance\", \"career\", \"reading\", \"idea\", \"experiment\", \"ml\", \"data\", \"ops\", \"writing\"\n]\n \nTOPIC_POOL = [\n    \"agent-architecture\", \"user-modeling\", \"product-vision\", \"growth\",\n    \"habit-tracking\", \"literature-review\", \"tutorials\", \"experiments\",\n    \"design-patterns\", \"notes-org\"\n]\n \nCONTEXT_POOL = [\"meeting\", \"journaling\", \"reading\", \"idea\", \"task-note\", \"clip\", \"email\"]\nSOURCE_POOL = [\"manual\", \"web-clip\", \"pdf\", \"auto-generated\", \"imported\"]\n \nINTERACTION_TYPES = [\n    \"open_note\", \"edit_note\", \"revisit_old_note\",\n    \"add_new_note\", \"search\", \"link_note\", \"merge_notes\"\n]\n \ndef random_timestamp(start=START_DATE, months_span=MONTHS_SPAN):\n    days = months_span * 30\n    offset = random.randint(0, days * 24 * 60 * 60)\n    return start + timedelta(seconds=offset)\n \ndef sample_tags(k=2):\n    k = min(k, len(TAG_POOL))\n    return sorted(random.sample(TAG_POOL, k))\n \ndef generate_paragraph(min_sent=2, max_sent=6):\n    return \" \".join(fake.sentence().rstrip('.') for _ in range(random.randint(min_sent, max_sent)))\n \n \n# ---- Notes ----\nnotes = []\nnote_ids = []\n \nfor _ in tqdm(range(N_NOTES), desc=\"Generating notes\"):\n    nid = str(uuid.uuid4())\n    note_ids.append(nid)\n \n    title = fake.sentence(nb_words=random.randint(3, 8)).rstrip('.')\n    paragraphs = [generate_paragraph() for _ in range(random.randint(1, 3))]\n    content = \"\\n\\n\".join(paragraphs)\n \n    n_tags = random.choices([1, 2, 3], weights=[0.5, 0.35, 0.15])[0]\n    tags = sample_tags(k=n_tags)\n    topic = random.choice(TOPIC_POOL)\n    context = random.choice(CONTEXT_POOL)\n    timestamp = random_timestamp()\n    source = random.choice(SOURCE_POOL)\n \n    # bias a few notes to be older for resurrection logic\n    if random.random() < 0.08:\n        timestamp = START_DATE + timedelta(days=random.randint(0, int(MONTHS_SPAN * 30 / 2)))\n \n    notes.append({\n        \"note_id\": nid,\n        \"title\": title,\n        \"content\": content,\n        \"tags\": \",\".join(tags),\n        \"topic\": topic,\n        \"context\": context,\n        \"timestamp\": timestamp,\n        \"source\": source\n    })\n \nnotes_df = pd.DataFrame(notes).sort_values(\"timestamp\").reset_index(drop=True)\n \n \n# ---- Interactions ----\ninteractions = []\ninteraction_id = 0\n \nif not notes_df.empty:\n    start_ts = notes_df[\"timestamp\"].min()\n    end_ts = notes_df[\"timestamp\"].max()\nelse:\n    start_ts = START_DATE\n    end_ts = datetime.utcnow()\n \nnum_sessions = max(30, N_NOTES // 4)\n \nfor _ in range(num_sessions):\n    session_anchor = start_ts + timedelta(\n        seconds=random.randint(0, int((end_ts - start_ts).total_seconds()))\n    )\n    session_length = random.randint(3, 8)\n \n    for _ in range(session_length):\n        interaction_id += 1\n        ts = session_anchor + timedelta(minutes=random.randint(0, 90))\n        itype = random.choices(\n            INTERACTION_TYPES,\n            weights=[0.3, 0.15, 0.1, 0.25, 0.08, 0.06, 0.06],\n            k=1\n        )[0]\n \n        target = random.choice(note_ids) if itype in [\"open_note\", \"edit_note\", \"revisit_old_note\", \"add_new_note\", \"link_note\", \"merge_notes\"] else \"\"\n \n        interactions.append({\n            \"interaction_id\": f\"int_{interaction_id}\",\n            \"type\": itype,\n            \"target_id\": target,\n            \"timestamp\": ts\n        })\n \ninteractions_df = pd.DataFrame(interactions).sort_values(\"timestamp\").reset_index(drop=True)\n \nprint(\"notes_df:\", notes_df.shape)\nprint(\"interactions_df:\", interactions_df.shape)\n \nnotes_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T19:20:58.456210Z","iopub.execute_input":"2025-11-30T19:20:58.456585Z","iopub.status.idle":"2025-11-30T19:20:58.612567Z","shell.execute_reply.started":"2025-11-30T19:20:58.456564Z","shell.execute_reply":"2025-11-30T19:20:58.611641Z"}},"outputs":[{"name":"stderr","text":"Generating notes: 100%|██████████| 400/400 [00:00<00:00, 4630.52it/s]","output_type":"stream"},{"name":"stdout","text":"notes_df: (400, 8)\ninteractions_df: (570, 4)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                note_id  \\\n0  b3fe0633-4805-4015-bf14-8976682a6296   \n1  175e9de5-6b57-4dee-97cf-21ad7a0a2ad3   \n2  49c02499-5c5f-4cb9-b7c9-0be3776143aa   \n3  b20750e6-1114-4df4-af5e-a7bf3729ddef   \n4  886a5879-e932-40de-b0ae-d6b2d948797b   \n\n                                         title  \\\n0  High tough hundred bar effect international   \n1                              Many most green   \n2                               Former bed use   \n3                     Question evening imagine   \n4                                Record couple   \n\n                                             content                   tags  \\\n0  Movie audience run yet Research nor positive m...                meeting   \n1  Center build happy near Trouble news five deci...                reading   \n2  Data character defense subject guy training To...           idea,writing   \n3  Site price ever Out wish fish determine adult ...           ops,personal   \n4  Owner company expert table reality stock site ...  idea,personal,product   \n\n               topic     context                  timestamp    source  \n0     habit-tracking        idea 2025-03-01 09:55:48.467780  imported  \n1     product-vision  journaling 2025-03-01 12:06:25.467780  imported  \n2        experiments     meeting 2025-03-03 08:58:51.467780  imported  \n3     habit-tracking     meeting 2025-03-04 09:43:32.467780       pdf  \n4  literature-review        idea 2025-03-05 05:33:51.467780       pdf  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>note_id</th>\n      <th>title</th>\n      <th>content</th>\n      <th>tags</th>\n      <th>topic</th>\n      <th>context</th>\n      <th>timestamp</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b3fe0633-4805-4015-bf14-8976682a6296</td>\n      <td>High tough hundred bar effect international</td>\n      <td>Movie audience run yet Research nor positive m...</td>\n      <td>meeting</td>\n      <td>habit-tracking</td>\n      <td>idea</td>\n      <td>2025-03-01 09:55:48.467780</td>\n      <td>imported</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>175e9de5-6b57-4dee-97cf-21ad7a0a2ad3</td>\n      <td>Many most green</td>\n      <td>Center build happy near Trouble news five deci...</td>\n      <td>reading</td>\n      <td>product-vision</td>\n      <td>journaling</td>\n      <td>2025-03-01 12:06:25.467780</td>\n      <td>imported</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>49c02499-5c5f-4cb9-b7c9-0be3776143aa</td>\n      <td>Former bed use</td>\n      <td>Data character defense subject guy training To...</td>\n      <td>idea,writing</td>\n      <td>experiments</td>\n      <td>meeting</td>\n      <td>2025-03-03 08:58:51.467780</td>\n      <td>imported</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b20750e6-1114-4df4-af5e-a7bf3729ddef</td>\n      <td>Question evening imagine</td>\n      <td>Site price ever Out wish fish determine adult ...</td>\n      <td>ops,personal</td>\n      <td>habit-tracking</td>\n      <td>meeting</td>\n      <td>2025-03-04 09:43:32.467780</td>\n      <td>pdf</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>886a5879-e932-40de-b0ae-d6b2d948797b</td>\n      <td>Record couple</td>\n      <td>Owner company expert table reality stock site ...</td>\n      <td>idea,personal,product</td>\n      <td>literature-review</td>\n      <td>idea</td>\n      <td>2025-03-05 05:33:51.467780</td>\n      <td>pdf</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"**Gemini Embeddings & Personal Knowledge Graph**\n \nNow we:\n \n1. Use Gemini embeddings to embed each note.\n2. Build a Personal Knowledge Graph (PKG) with:\n   - note nodes\n   - tag nodes\n   - semantic edges (similar notes)\n   - temporal edges (notes created near each other)\n   - interaction-based metadata (revisits, last interaction)\n3. Create a UMAP projection for visualization.","metadata":{}},{"cell_type":"code","source":"# ==== Embeddings with Gemini + Graph Construction ====\n \nnotes_df[\"timestamp\"] = pd.to_datetime(notes_df[\"timestamp\"])\nnotes_df[\"content_short\"] = notes_df[\"title\"].fillna('') + \". \" + notes_df[\"content\"].fillna('')\n \ntexts = notes_df[\"content_short\"].tolist()\n \ndef embed_texts_with_gemini(text_list, model_name=EMBED_MODEL, batch_size=32):\n    \"\"\"Use Gemini embeddings API to embed a list of texts.\"\"\"\n    all_embeddings = []\n    for i in tqdm(range(0, len(text_list), batch_size), desc=\"Embedding notes with Gemini\"):\n        batch = text_list[i:i+batch_size]\n        result = client.models.embed_content(\n            model=model_name,\n            contents=batch,\n            config=types.EmbedContentConfig(\n                task_type=\"retrieval_document\",\n            ),\n        )\n        # result['embedding'] is a list of vectors (for list input)\n        batch_embs = result.embeddings\n        print(batch_embs)\n        all_embeddings.extend(batch_embs)\n        print(all_embeddings)\n    return np.array(all_embeddings, dtype=\"float32\")\n \nemb_matrix = embed_texts_with_gemini(texts)\nemb_matrix = normalize(emb_matrix)\n \nprint(\"Embedding matrix shape:\", emb_matrix.shape)\n \n# Mapping note_id <-> embedding index\nnote_id_to_idx = {nid: i for i, nid in enumerate(notes_df[\"note_id\"])}\nidx_to_note_id = {i: nid for nid, i in note_id_to_idx.items()}\n \n# Full cosine similarity matrix\nsim_matrix = cosine_similarity(emb_matrix)\nprint(\"sim_matrix shape:\", sim_matrix.shape)\n \n# Build NetworkX graph\nG = nx.Graph()\n \n# Add note nodes\nfor _, row in notes_df.iterrows():\n    nid = f\"note:{row['note_id']}\"\n    G.add_node(\n        nid,\n        node_type=\"note\",\n        title=row[\"title\"],\n        content=row[\"content\"],\n        tags=row[\"tags\"],\n        topic=row[\"topic\"],\n        context=row[\"context\"],\n        timestamp=row[\"timestamp\"],\n        source=row[\"source\"]\n    )\n \n# Add semantic edges (top-K neighbors beyond threshold)\nTOP_K = 6\nSIM_THRESHOLD = 0.55\n \nfor i in range(len(notes_df)):\n    nid_i = notes_df.loc[i, \"note_id\"]\n    src_key = f\"note:{nid_i}\"\n    sims = sim_matrix[i]\n    neighbor_idx = np.argsort(-sims)[: TOP_K + 1]  # includes self\n \n    for j in neighbor_idx:\n        if j == i:\n            continue\n        score = float(sims[j])\n        if score < SIM_THRESHOLD:\n            continue\n        nid_j = idx_to_note_id[j]\n        dst_key = f\"note:{nid_j}\"\n \n        if G.has_edge(src_key, dst_key):\n            if score > G[src_key][dst_key].get(\"weight\", 0):\n                G[src_key][dst_key][\"weight\"] = score\n                G[src_key][dst_key][\"type\"] = \"semantic\"\n        else:\n            G.add_edge(src_key, dst_key, type=\"semantic\", weight=score)\n \n# Add tag nodes & edges\nfor _, row in notes_df.iterrows():\n    nid = f\"note:{row['note_id']}\"\n    tags_str = row.get(\"tags\", \"\")\n    if pd.isna(tags_str) or str(tags_str).strip() == \"\":\n        continue\n    for t in str(tags_str).split(\",\"):\n        t = t.strip()\n        if not t:\n            continue\n        tid = f\"tag:{t}\"\n        if not G.has_node(tid):\n            G.add_node(tid, node_type=\"tag\", tag=t)\n        G.add_edge(nid, tid, type=\"shares_tag\")\n \n# Add simple temporal edges: notes within 3 days of each other (sliding window)\nnotes_ts = notes_df[[\"note_id\", \"timestamp\"]].dropna().sort_values(\"timestamp\").reset_index(drop=True)\n \nfor i in range(len(notes_ts)):\n    for j in range(i + 1, min(i + 4, len(notes_ts))):\n        t1 = notes_ts.loc[i, \"timestamp\"]\n        t2 = notes_ts.loc[j, \"timestamp\"]\n        if abs((t2 - t1).days) <= 3:\n            n1 = f\"note:{notes_ts.loc[i, 'note_id']}\"\n            n2 = f\"note:{notes_ts.loc[j, 'note_id']}\"\n            if not G.has_edge(n1, n2):\n                G.add_edge(n1, n2, type=\"created_near\", weight=0.2)\n \n# Interactions to mark revisits / last interaction\ninteractions_df[\"timestamp\"] = pd.to_datetime(interactions_df[\"timestamp\"])\nfor _, r in interactions_df.iterrows():\n    itype = r[\"type\"]\n    target = r[\"target_id\"]\n    if not target:\n        continue\n    if target in note_id_to_idx:\n        nid = f\"note:{target}\"\n        if nid not in G:\n            continue\n        if itype == \"revisit_old_note\":\n            G.nodes[nid][\"revisited\"] = G.nodes[nid].get(\"revisited\", 0) + 1\n        if itype in [\"open_note\", \"edit_note\", \"add_new_note\"]:\n            G.nodes[nid][\"last_interaction\"] = r[\"timestamp\"]\n \nprint(\"Graph nodes:\", G.number_of_nodes())\nprint(\"Graph edges:\", G.number_of_edges())\n \n# UMAP projection\nreducer = umap.UMAP(n_components=2, random_state=RANDOM_SEED)\nemb_2d = reducer.fit_transform(emb_matrix)\nnotes_df[\"umap_x\"] = emb_2d[:, 0]\nnotes_df[\"umap_y\"] = emb_2d[:, 1]\n \nplt.scatter(notes_df[\"umap_x\"], notes_df[\"umap_y\"], s=10)\nplt.title(\"UMAP projection of note embeddings (Gemini)\")\nplt.show()\n \nnotes_meta = notes_df.set_index(\"note_id\")[[\"timestamp\", \"tags\", \"topic\", \"umap_x\", \"umap_y\"]]\nnotes_meta.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T19:41:50.088707Z","iopub.execute_input":"2025-11-30T19:41:50.089045Z","iopub.status.idle":"2025-11-30T19:41:51.070387Z","shell.execute_reply.started":"2025-11-30T19:41:50.089020Z","shell.execute_reply":"2025-11-30T19:41:51.069364Z"}},"outputs":[{"name":"stderr","text":"Embedding notes with Gemini:   0%|          | 0/13 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/3909873004.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0memb_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_texts_with_gemini\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0memb_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/3909873004.py\u001b[0m in \u001b[0;36membed_texts_with_gemini\u001b[0;34m(text_list, model_name, batch_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# result['embedding'] is a list of vectors (for list input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mbatch_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mall_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"],"ename":"AttributeError","evalue":"'list' object has no attribute 'values'","output_type":"error"}],"execution_count":23}]}